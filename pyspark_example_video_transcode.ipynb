# 1.Forge Pyspark: Prediction of transcoding time for videos

*Input and output video characteristics along with their transcoding time and memory resource requirements while transcoding videos to different but valid formats. The goal here is to predict the transcoding time based on video features and allocated memory. A detailed explanation of the data can be found here* 
1. Cluster: HDP1 (DEV) DB: 	hce_dev_xsbg_published_dstraining
2. Train_Data_Table:  	training_transcoding (used to train and evaluate model)
3. Test_Data_Table: 	test_transcoding  (true label are removed. You need to make prediction on this test data, and save the prediction result to result table)
4. Result_Table:  	result_transcoding  (Put your own hid in column ‘user_id’)

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from time import time
%matplotlib inline

## 1. Connect to pyspark

from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
spark = SparkSession.builder.config(conf=SparkConf()\
                                    .setMaster("yarn")\
                                    .setAppName("DominoPysparkTrain")\
                                    .set("spark.executor.memory", '2g')\
                                    .set('spark.executor.cores', '3')).getOrCreate()

spark

## 2. Get data from hive

df_train = spark.sql('select * from hce_dev_xsbg_published_dstraining.training_transcoding')

df_train.show()

df_train.printSchema()
print("There are {} columns".format(len(df_train.columns)))

df_test = spark.sql('select * from hce_dev_xsbg_published_dstraining.test_transcoding')

df_test.show()

df_test.printSchema()
print("There are {} columns".format(len(df_test.columns)))

df_train.count(),df_test.count()

df_train.na.drop().count(), df_test.na.drop().count()

In the above block the count of records before and after dropping NA are same - It means that there are no NULL records in the dataset

## 3. Exploratary data analysis

### Cached the dataframe for faster processing - using the benefits of spark 

df_train = df_train.cache()

#check all the distinct counts
from pyspark.sql.functions import col, countDistinct
for c in df_train.columns:
    cnt = df_train.select(c).distinct().count()
    print(c, cnt)

The distinct count in the above table shows that *b_size distinct value = 1*. That means that it has constant value 
and it will not be useful in model building. Hence we should eliminate it. Also id is just an identifier and we can elimiate that too. We will do it later on.

df_train.show(2)

df_train.groupBy("codec").count().show()

df_train.groupBy("o_codec").count().show()

#lets see how input and output codec are interacting
#create a new column to define interaction
from pyspark.sql import functions as sf
df_train = df_train.withColumn('io_codec', 
                    sf.concat(sf.col('codec'),sf.lit('_'), sf.col('o_codec')))

df_train.groupBy("io_codec").count().show(30)

#it seems that there are transcoding from and to at same codec...h264_h264 etc 
#probably some other parameters like framerate and bit rates may be different for those
#lets check
df_train.filter(sf.col("io_codec").isin(["h264_h264"])).show(10)

The above table shows that all the input side data is same but the output expectations are different for the transcoding to the same codec. Measures like, output framerate, width and heights etc are different. That results in different decoding time


#lets see if there are specific groups of height, width, framerate and bitrates
#these columns are numeric however they seem whole numbers
df_train.groupBy("width").count().show()
df_train.groupBy("o_width").count().show()

df_train.groupBy("height").count().show()
df_train.groupBy("o_height").count().show()

#check the same with test data
df_test.groupBy("width").count().show()
df_test.groupBy("o_width").count().show()

df_test.groupBy("height").count().show()
df_test.groupBy("o_height").count().show()

The above count tables for height and width category show that height and width is like a resolution, for exampe 480 x 1080 etc
Hence we can create an interaction measure by combining both these variables together

df_train = df_train.withColumn('ihxw', 
                    sf.concat(sf.col('height'),sf.lit('x'), sf.col('width')))

df_train = df_train.withColumn('ohxw', 
                    sf.concat(sf.col('o_height'),sf.lit('x'), sf.col('o_width')))

df_train.groupBy("ihxw").count().show()
df_train.groupBy("ohxw").count().show()

#create the final table that depicts the interactions
df_train = df_train.withColumn('iohxw', 
                    sf.concat(sf.col('ihxw'),sf.lit('_'), sf.col('ohxw')))

df_train.groupBy("iohxw").count().show(100)

#lets create these new columns for test dataset as well.
df_test = df_test.withColumn('io_codec', 
                    sf.concat(sf.col('codec'),sf.lit('_'), sf.col('o_codec')))
df_test = df_test.withColumn('ihxw', 
                    sf.concat(sf.col('height'),sf.lit('x'), sf.col('width')))
df_test = df_test.withColumn('ohxw', 
                    sf.concat(sf.col('o_height'),sf.lit('x'), sf.col('o_width')))
df_test = df_test.withColumn('iohxw', 
                    sf.concat(sf.col('ihxw'),sf.lit('_'), sf.col('ohxw')))
df_test.groupBy("iohxw").count().show(100)

### List of numerical features 
we can calculate pearson's correlation. 
Notice that all the column data type in this hive table is originally string, 
we need to manually change the data type of those numerical columns first

numerical_columns = ['duration', 'bitrate', 'framerate', 'i', 'p', 'b', 'frames',
 'i_size', 'p_size', 'size', 'o_bitrate', 'o_framerate', 'umem',
 'utime']

df_train_numerical=df_train.select(*[col(x).cast('float') for x in numerical_columns])

df_train_numerical.select('i_size', 'p_size', 'size').show(5)

df_train_numerical.select('i', 'p', 'b', 'frames').show(5)

The above table shows that the *size* is the sum of *i_size and p_size*,  (b_size is constant = 0) and *frames* is the sum of *i, p and b*. 

#lets look at the transcoding time description - especially checking of the values are zero or less
df_train_numerical.describe("utime").show()

!pip install pyspark_dist_explore

from pyspark_dist_explore import hist , distplot
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
hist(ax, df_train_numerical[['utime']], bins = 5)

import seaborn as sns
x = df_train_numerical.select('utime').toPandas()

#plt.hist(x, bins = 5,logx=True)
#plt.show()
sns.boxplot(data=x)

The *utime* values seem right skewed and most of the transcoding time is less than 50

model_param_transform_to_log = {}  # dict for information of parameters that are converted to log

#creating a new utime variable with log
from pyspark.sql import functions as F
df_train_numerical = df_train_numerical.withColumn('logutime', F.log1p(df_train_numerical["utime"]) )

fig, ax = plt.subplots()
hist(ax, df_train_numerical[['logutime']], bins = 5)

model_param_transform_to_log['utime'] = 'logutime'

df_train_numerical.describe("bitrate", "o_bitrate").show()

df_train_numerical.groupBy("bitrate", "o_bitrate").count().show()

There are many records where the bitrate transformation during transcoding are unique (count =1).

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["bitrate", "o_bitrate"]])

Since the *bitrate* values are exteremely large and the histogram is right skewed hence we can explore the log transformation of these features

from pyspark.sql import functions as F
df_train_numerical = df_train_numerical.withColumn('logbitrate', F.log1p(df_train_numerical["bitrate"]) )
df_train_numerical = df_train_numerical.withColumn('logo_bitrate', F.log1p(df_train_numerical["o_bitrate"]) )

model_param_transform_to_log['bitrate'] = 'logbitrate'
model_param_transform_to_log['o_bitrate'] = 'logo_bitrate'

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["logbitrate", "logo_bitrate"]])

df_train_numerical.describe("framerate", "o_framerate").show()

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["framerate", "o_framerate"]])

framerates seem normal. There is no need for any transformation

#lets see if subrtacting framerates would be a good idea
df_train_numerical = df_train_numerical.withColumn('framerate_diff', ( 
                        df_train_numerical['o_framerate'] - df_train_numerical['framerate'] ))

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["framerate_diff"]])

The framerate difference feature could be useful.

#lets see if subrtacting bitrates and framerates would be a good idea
df_train_numerical = df_train_numerical.withColumn('logbitrate_diff', ( 
                        df_train_numerical['logo_bitrate'] - df_train_numerical['logbitrate'] ))

fig, ax = plt.subplots()
hist(ax, df_train_numerical[['logbitrate_diff']])

the bitrate difference feature could be useful

df_train_numerical.describe("i", "p", "b", "frames").show()

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["i", "p", "b", "frames"]])

The *"i", "p", "b", "frames"* are right skewed and have very large values. Hence log transformation could be a good option

df_train_numerical = df_train_numerical.withColumn('logi', F.log1p(df_train_numerical["i"]) )
df_train_numerical = df_train_numerical.withColumn('logp', F.log1p(df_train_numerical["p"]) )
df_train_numerical = df_train_numerical.withColumn('logb', F.log1p(df_train_numerical["b"]) )
df_train_numerical = df_train_numerical.withColumn('logframes', F.log1p(df_train_numerical["frames"]) )

model_param_transform_to_log['i'] = 'logi'
model_param_transform_to_log['p'] = 'logp'
model_param_transform_to_log['b'] = 'logb'
model_param_transform_to_log['frames'] = 'logframes'

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["logi", "logp", "logb", "logframes"]])

df_train_numerical.describe("i_size", "p_size", "size").show()

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["i_size", "p_size", "size"]])

*"i_size", "p_size", "size"* have very large values and the distribution is right skewed. Hence log transformation would be useful

df_train_numerical = df_train_numerical.withColumn('logi_size', F.log1p(df_train_numerical["i_size"]) )
df_train_numerical = df_train_numerical.withColumn('logp_size', F.log1p(df_train_numerical["p_size"]) )
df_train_numerical = df_train_numerical.withColumn('logsize', F.log1p(df_train_numerical["size"]) )

model_param_transform_to_log['i_size'] = 'logi_size'
model_param_transform_to_log['p_size'] = 'logp_size'
model_param_transform_to_log['size'] = 'logsize'

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["logi_size", "logp_size", "logsize"]])

df_train_numerical.describe("duration").show()

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["duration"]])

df_train_numerical = df_train_numerical.withColumn('logduration', F.log1p(df_train_numerical["duration"]) )

model_param_transform_to_log['duration'] = 'logduration'

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["logduration"]])

df_train_numerical.describe("umem").show()

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["umem"]])

The *umem* distribution doesn't look too skewed. However it has very large values and it is little bit right skewed too. Hence log transformation could help here.

df_train_numerical = df_train_numerical.withColumn('logumem', F.log1p(df_train_numerical["umem"]) )

model_param_transform_to_log['umem'] = 'logumem'

fig, ax = plt.subplots()
hist(ax, df_train_numerical[["logumem"]])

### List of final Numerical features

for k, v in model_param_transform_to_log.items():
    print (k, v)

numerical_columns

### Transforming the testing dataset features to log1p

#create these parameters for test set
numerical_columns.remove( 'utime' )
df_test_numerical=df_test.select(*[col(x).cast('float') for x in numerical_columns])

df_test_numerical.describe().show()

for k, v in model_param_transform_to_log.items():
    if k in numerical_columns:
        df_test_numerical = df_test_numerical.withColumn(v, F.log1p(df_test_numerical[k]) )

df_test_numerical = df_test_numerical.withColumn('logbitrate_diff', ( 
                        df_test_numerical['logo_bitrate'] - df_test_numerical['logbitrate'] ))

df_test_numerical = df_test_numerical.withColumn('framerate_diff', ( 
                        df_test_numerical['o_framerate'] - df_test_numerical['framerate'] ))

df_test_numerical.columns

keep_numerical_columns = [
 'framerate',
 'o_framerate',
 'logumem',
 'logutime',
 'logbitrate',
 'logo_bitrate',
 'framerate_diff',
 'logbitrate_diff',
 'logi',
 'logp',
 'logb',
 'logframes',
 'logi_size',
 'logp_size',
 'logsize',
 'logduration']

df_train_numerical = df_train_numerical[keep_numerical_columns]

keep_numerical_columns_test = list (set(keep_numerical_columns) - set(['logutime']))
df_test_numerical = df_test_numerical[keep_numerical_columns_test]

len(df_train_numerical.columns), len(df_test_numerical.columns)

In order to use the Correlation function in pyspark, the input dataset should be the form of Vectors. A feature transformer that merges multiple columns into a vector column. VectorAssembler can be used to merge multiple columns into a vector column.

from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols=keep_numerical_columns, outputCol = "numerical_features")
df_train_num = assembler.transform(df_train_numerical).select("numerical_features")
df_train_num.take(2)

Notice that DenseVector is automatically used as there are NOT many 0 values in the data.

from pyspark.ml.stat import Correlation

t0 = time()
personCorr = Correlation.corr(df_train_num, 'numerical_features', 'pearson').collect()[0][0]
tt = time() - t0
print ("Calculating correlation matrix performed in {} seconds".format(round(tt,3)))

corr_df = pd.DataFrame(personCorr.toArray(), index=keep_numerical_columns, columns=keep_numerical_columns)
plt.figure(figsize=(10,10))
plt.rc('xtick',labelsize=10)
plt.rc('ytick',labelsize=10)
sns.heatmap(corr_df)

# get a boolean dataframe where true means that a pair of variables is highly correlated
highly_correlated_df = (corr_df.abs() > .85) & (corr_df.abs() <= 1.0)

# get the names of the variables so we can use them to slice the dataframe
correlated_vars_index = (highly_correlated_df==True).any()
correlated_var_names = correlated_vars_index[correlated_vars_index==True].index

de_duplicate = []

correlated_pairs = []
# slice it
for i in correlated_vars_index.index:
    row = highly_correlated_df[i]
    de_duplicate.append(i)
    for j in correlated_var_names:
        if j not in de_duplicate and row[j] == True:
            print(i,j,":",corr_df.loc[i,j])
            correlated_pairs.append((i,j))

We will remove the highly co related features from the pairs and keep only one of them.

print("Original numer of numercial features:", len(keep_numerical_columns))
for pair in correlated_pairs:
    if pair[0] in keep_numerical_columns:
        keep_numerical_columns.remove(pair[0])
print("Number of numerical features after removing correlated ones:", len(keep_numerical_columns))

keep_numerical_columns

### List of categorical columns
This will include some of the new columns that we have constructed

cat_columns = ['io_codec', 'iohxw']

Y_column = 'logutime'

### Create the dataframes with final features list

df_train_final = spark.createDataFrame(pd.concat(
    [ df_train[cat_columns].toPandas(), df_train_numerical[keep_numerical_columns].toPandas() ], 
    axis=1) )

keep_numerical_columns_test = keep_numerical_columns
keep_numerical_columns_test.remove(Y_column)
df_test_final = spark.createDataFrame(pd.concat(
    [ df_test[cat_columns].toPandas(), df_test_numerical[keep_numerical_columns_test].toPandas() ], 
    axis=1) )

df_train_final.printSchema()

df_test_final.printSchema()

### Create one hot encoding

from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoderEstimator
stages = [] 
for c in cat_columns:
    stringIndexer = StringIndexer(inputCol= c, outputCol = c + 'Index', handleInvalid = 'keep') 
    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[c + "classVec"],handleInvalid = 'keep')
    stages += [stringIndexer, encoder]

### Merge features needed into vector

assemblerInputs = [c + "classVec" for c in cat_columns] + keep_numerical_columns_test
assembler = VectorAssembler(inputCols=assemblerInputs, outputCol="features")
stages += [assembler]

### Set pipeline for running the regression model

from pyspark.ml import Pipeline

t0 = time()
pipeline = Pipeline(stages = stages)
pipelineModel = pipeline.fit(df_train_final)
df_train_final = pipelineModel.transform(df_train_final)

selectedCols = ['logutime', 'features'] 
df_train_ready = df_train_final.select(selectedCols)
df_train_ready.printSchema()

tt = time() - t0
print ("Pipeline built in {} seconds".format(round(tt,3)))

df_train_ready.show()

# Linear Regression Model

#Approximate running time: 56 seconds, without caching: 120 seconds

from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression

t0 = time()
lr = LinearRegression(featuresCol = 'features', labelCol='logutime', maxIter=10, fitIntercept=False)
#lr = GeneralizedLinearRegression(family="Gamma", link="Log", maxIter=10, labelCol='logutime', regParam=0.3)
lrModel = lr.fit(df_train_ready)
tt = time() - t0

print ("Classifier trained in {} seconds".format(round(tt,3)))

## Train accuracy

print("Coefficients: %s" % str(lrModel.coefficients))
print("Intercept: %s" % str(lrModel.intercept))

#Approximate running time: 75 seconds, without caching: 184 seconds
t0 = time()
trainingSummary = lrModel.summary
print('Training data accuracy: ' + str(trainingSummary.r2))
print('Training data explainedVariance: ' + str(trainingSummary.explainedVariance))
print('Training data meanSquaredError: ' + str(trainingSummary.meanSquaredError))
print('Training data meanAbsoluteError: ' + str(trainingSummary.meanAbsoluteError))
tt = time() - t0
print ("Training summary calculated in {} seconds".format(round(tt,3)))

#Approximate running time: 75 seconds, without caching: 156 seconds
t0 = time()
predictions = lrModel.transform(df_train_ready )
#convert them to expm1
predictions = predictions.withColumn('expm1prediction', F.expm1(predictions["prediction"]) )
predictions.show(10)
tt = time() - t0
print ("Making predictions performed in {} seconds".format(round(tt,3)))

df = predictions.filter(predictions.prediction <= 0)
df.count()

df.describe().show()

The predicted values shouldn't go below 0. Hence setting all negative values to min = 0.2


predictions = predictions.withColumn("prediction", F.when(F.col("prediction")<=0, 0.2).otherwise(F.col("prediction")))
predictions = predictions.withColumn('expm1prediction', F.expm1(predictions["prediction"]) )
predictions.show(10)

fig, ax = plt.subplots()
hist(ax, predictions[["logutime", "prediction"]])

predictions.count()

# visualize predictions and actual values in log-log space
from matplotlib.colors import ListedColormap, Normalize
npts = 58510

logpreds = [row['prediction'] for row in predictions.take(npts)]
loglabs = [row['logutime'] for row in predictions.take(npts)]
err = [abs(logpreds[i]-l) for i,l in enumerate(loglabs)]

fig = plt.figure()
plt.scatter(logpreds, loglabs, s=8**2, c=['red', 'green'], edgecolors='#888888', alpha=0.75, linewidths=.5)
plt.plot((-1,6),(-1,6), linestyle='--', color='#555555')
#plt.xlim((6.5,9))
plt.xlabel('log(Predicted)')
plt.ylabel('log(Actual)')
plt.title('Median absolute error: %.3f' % np.median(err))
#display(fig)

### Building pipeline for Test dataset

t0 = time()
pipeline = Pipeline(stages = stages)
pipelineModel = pipeline.fit(df_test_final)
df_test_final = pipelineModel.transform(df_test_final)

selectedCols = ['features'] 
df_test_ready = df_test_final.select(selectedCols)
df_test_ready.printSchema()

tt = time() - t0
print ("Pipeline built in {} seconds".format(round(tt,3)))

df_test_ready.show()

## Test accuracy

#Approximate running time: 75 seconds, without caching: 156 seconds
t0 = time()
predictions_test = lrModel.transform(df_test_ready)
#convert them to expm1
predictions_test = predictions_test.withColumn('expm1prediction', F.expm1(predictions_test["prediction"]) )
predictions_test.show(10)
tt = time() - t0
print ("Making predictions performed in {} seconds".format(round(tt,3)))

df = predictions_test.filter(predictions_test.prediction <= 0)
df.count()

#reset all zero and negative predictions
predictions_test = predictions_test.withColumn("prediction", F.when(F.col("prediction")<=0, 0.2).otherwise(F.col("prediction")))
predictions_test = predictions_test.withColumn('expm1prediction', F.expm1(predictions_test["prediction"]) )
df = predictions_test.filter(predictions_test.prediction <= 0)
df.count()

f, axes = plt.subplots(1,2, figsize=(20,10))
f.tight_layout()
npts = 10000
logtrainpreds = [row['prediction'] for row in predictions.take(npts)]
logtestpreds = [row['prediction'] for row in predictions_test.take(npts)]
axes[0].hist(logtrainpreds)
axes[0].set_title('log Histogram of train utime predictions')
axes[1].hist(logtestpreds)
axes[1].set_title('log Histogram of test utime predictions')
#display(f)

predictions_test.count()

# visualize predictions and actual values in log-log space
from matplotlib.colors import ListedColormap, Normalize
npts = 1000

logpreds = [row['prediction'] for row in predictions_test.take(npts)]
loglabs = [i for i, row in enumerate(predictions_test.take(npts))]

fig = plt.figure()
plt.scatter(loglabs, logpreds, s=8**2, c=['red', 'green'], edgecolors='#888888', alpha=0.75, linewidths=.5)
#plt.plot((-1,6),(-1,6), linestyle='--', color='#555555')
#plt.xlim((6.5,9))
plt.xlabel('index')
plt.ylabel('log(Actual)')
plt.title('test plot')
#display(fig)

# Decision Tree Regression Model

from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression, DecisionTreeRegressor
dt = DecisionTreeRegressor(featuresCol = 'features', labelCol = 'logutime')
dtModel = dt.fit(df_train_ready)

# Make predictions.
predictions = dtModel.transform(df_train_ready )
predictions.select("features","logutime","prediction").show(5)

#check -ve values
df = predictions.filter(predictions.prediction <= 0)
df.count()

from pyspark.ml.evaluation import RegressionEvaluator
# Select (prediction, true label) and compute test error
evaluator = RegressionEvaluator(labelCol="logutime",
                                predictionCol="prediction",
                                metricName="rmse")

rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)

The error is More than Linear regression

y_true = predictions.select("logutime").toPandas()
y_pred = predictions.select("prediction").toPandas()

import sklearn.metrics
r2_score = sklearn.metrics.r2_score(y_true, y_pred)
print('r2_score: {0}'.format(r2_score))

The R2 score is also lesser than regression

### Prediction on Test data

predictions_test = dtModel.transform(df_test_ready )
predictions_test.show(5)

#check -ve values
df = predictions_test.filter(predictions_test.prediction <= 0)
df.count()

There is no prediction where the value is either 0 or negative. The Linear Regression resulted in a few negative values though.

# Gradient Boost Model

from pyspark.ml.regression import GBTRegressor
gbt = GBTRegressor(maxIter=200, featuresCol = 'features', labelCol = 'logutime')
gbModel = gbt.fit(df_train_ready)

predictions = gbModel.transform(df_train_ready )
predictions.select("features","logutime","prediction").show(5)

evaluator = RegressionEvaluator(
    labelCol="logutime", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)

#check -ve values
df = predictions.filter(predictions.prediction <= 0)
df.count()

df.describe().show()

y_true = predictions.select("logutime").toPandas()
y_pred = predictions.select("prediction").toPandas()

import sklearn.metrics
r2_score = sklearn.metrics.r2_score(y_true, y_pred)
print('r2_score: {0}'.format(r2_score))

RMSE is lowest and R2 is highest in the Gradient boosted model
The count when the value is less than 0 is just 1 record

df = predictions.filter(predictions.prediction > 0)
df.describe().show()

### Prediction on test data

predictions_test = gbModel.transform(df_test_ready )
predictions_test.describe().show()

#check -ve values
df = predictions_test.filter(predictions_test.prediction <= 0)
df.count()

#reset all zero and negative predictions
replace_neg_values_with = 0.02
predictions_test = predictions_test.withColumn("prediction", F.when(F.col("prediction")<=0.001, replace_neg_values_with).otherwise(F.col("prediction")))
predictions_test = predictions_test.withColumn('expm1prediction', F.expm1(predictions_test["prediction"]) )
predictions_test.describe().show()

predictions_test.printSchema()

#### More Models can be generated and comparision can be made. For now lets pick GB model and write results in HIVE table

# Writing to HIVE Table

### There are issues related to casting - there are index in the test dataset that are very long, i.e. 8589939398, and casting doesn't work properly there. Resolved it after lot of iterations
### Multiple times the checks were done for NULL ness and Duplicates in order to ensure that the written data is correct. Hence the code is a bit repetitive

#write result to hive table
df_result = spark.sql('select * from hce_dev_xsbg_published_dstraining.result_transcoding')
df_result.printSchema()

predictions_test.printSchema()

temp = predictions_test.select('expm1prediction', 'prediction')

temp.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for c in temp.columns]).show()

No NULL values in Predictions_test

temp = df_test.select('index')
#df_test[['index']].toPandas().to_csv("df_test_index.csv")
temp.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for c in temp.columns]).show()

No NULL values in index

#check duplicates
print(temp.count(), temp.dropDuplicates(['index']).count())

temp.printSchema()

#from pyspark.sql.types import StringType, IntegerType, FloatType
#temp = temp.withColumn("index",temp["index"].cast("decimal(38, 0)")) # decimal is used instead of Integer because of long type

#check duplicates
#print(temp.count(), temp.dropDuplicates(['index']).count())
#temp[['index']].toPandas().to_csv("df_test_index_cast_int.csv")

#adding columns from two different datasets to one dataset
temp = spark.createDataFrame(pd.concat(
    [ df_test[['index']].toPandas(), predictions_test[['expm1prediction']].toPandas() ], 
    axis=1) )

temp.printSchema()

temp.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for c in temp.columns]).show()

No NULL value after aggregation

print(temp.count(), temp.dropDuplicates(['index']).count())

#https://stackoverflow.com/questions/38979733/cast-a-very-long-string-as-an-integer-or-long-integer-in-pyspark
temp = temp.withColumn("index", col("index").cast("decimal(38, 0)")) # decimal is used instead of Integer because of long value
temp = temp.withColumn("prediction", col("expm1prediction").cast("float")) #rename
temp = temp.drop("expm1prediction") #remove

print(temp.count(), temp.dropDuplicates(['index']).count())

#temp = temp.withColumn("index", col("index").cast("integer")) #first float conversion in above cell then int conversion in this cell

temp.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for c in temp.columns]).show()

No NULL values observed

from pyspark.sql.functions import lit
result = temp.select('index', 'prediction')
result = result.withColumn('user_id',lit("e208058"))
result.show(25)

result.columns

from pyspark.sql.functions import isnan, when, count, col
result.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for c in result.columns]).show()

No NULL values observed in final table

result.printSchema()

from pyspark.sql import HiveContext

hiveContext = HiveContext(spark)
hiveContext.setConf("hive.exec.dynamic.partition", "true")
hiveContext.setConf("hive.exec.dynamic.partition.mode", "nonstrict")

#write result to hive table
t0 = time()

result.write.insertInto("hce_dev_xsbg_published_dstraining.result_transcoding",\
                        overwrite = True)

tt = time() - t0
print ("Write prediction result to hive table performed in {} seconds".format(round(tt,3)))

query = "select count(*) from hce_dev_xsbg_published_dstraining.result_transcoding \
where user_id = '{}'".format("e208058")
spark.sql(query).show()

query = "select * from hce_dev_xsbg_published_dstraining.result_transcoding \
where user_id = '{}'".format("e208058")
df_result_from_DB = spark.sql(query)
df_result_from_DB.printSchema()

df_result_from_DB.count()

df_result_from_DB.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for c in df_result_from_DB.columns]).show()

No NULL records in the DB. Hence the writing to DB is mostly correct! Ah!

spark.stop()
